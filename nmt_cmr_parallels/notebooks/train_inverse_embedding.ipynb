{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in locals() else os.getcwd()\n",
    "root_dir = current_dir\n",
    "while not os.path.isdir(os.path.join(root_dir, \"nmt_cmr_parallels\")):\n",
    "    parent_dir = os.path.dirname(root_dir)\n",
    "    if parent_dir == root_dir:\n",
    "        raise FileNotFoundError(\"Could not find 'nmt_cmr_parallels' folder in any parent directories\")\n",
    "    root_dir = parent_dir\n",
    "\n",
    "# Add project root to sys.path\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "from nmt_cmr_parallels.data.sequence_data import load_cached_vocabulary, create_pretrained_semantic_embedding\n",
    "DEFAULT_DATA_PATH = os.path.expanduser(os.path.join('~', '.seq_nlp_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_tokens=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = create_pretrained_semantic_embedding(DEFAULT_DATA_PATH,50)\n",
    "vocab = load_cached_vocabulary(\"peers_vocab.json\")\n",
    "vocab = [x.lower() for x in vocab]\n",
    "if with_tokens:\n",
    "    vocab = ['<null>'] + vocab + ['<SoS>','<EoS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(len(vocab), 50)\n",
    "    \n",
    "# Initialize the weights of the Embedding layer with the GloVe vectors\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "for word, index in word_to_index.items():\n",
    "    if word in pretrained_embedding:\n",
    "        embedding_layer.weight.data[index] = torch.tensor(pretrained_embedding[word], dtype=torch.float32)\n",
    "\n",
    "# Freeze the embedding layer\n",
    "for param in embedding_layer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 0, Loss: 7.43808650970459\n",
      "Epoch 2/200, Step 0, Loss: 7.330111503601074\n",
      "Epoch 3/200, Step 0, Loss: 7.072533130645752\n",
      "Epoch 4/200, Step 0, Loss: 6.789166450500488\n",
      "Epoch 5/200, Step 0, Loss: 6.608606815338135\n",
      "Epoch 6/200, Step 0, Loss: 6.20540714263916\n",
      "Epoch 7/200, Step 0, Loss: 5.933229446411133\n",
      "Epoch 8/200, Step 0, Loss: 5.491092681884766\n",
      "Epoch 9/200, Step 0, Loss: 5.146666049957275\n",
      "Epoch 10/200, Step 0, Loss: 4.6481428146362305\n",
      "Epoch 11/200, Step 0, Loss: 4.003405570983887\n",
      "Epoch 12/200, Step 0, Loss: 3.7342138290405273\n",
      "Epoch 13/200, Step 0, Loss: 3.2699272632598877\n",
      "Epoch 14/200, Step 0, Loss: 3.016238212585449\n",
      "Epoch 15/200, Step 0, Loss: 2.2846860885620117\n",
      "Epoch 16/200, Step 0, Loss: 1.8921103477478027\n",
      "Epoch 17/200, Step 0, Loss: 1.6490181684494019\n",
      "Epoch 18/200, Step 0, Loss: 1.6009495258331299\n",
      "Epoch 19/200, Step 0, Loss: 1.089294672012329\n",
      "Epoch 20/200, Step 0, Loss: 1.0453386306762695\n",
      "Epoch 21/200, Step 0, Loss: 0.9235116243362427\n",
      "Epoch 22/200, Step 0, Loss: 0.7900962233543396\n",
      "Epoch 23/200, Step 0, Loss: 0.6075543165206909\n",
      "Epoch 24/200, Step 0, Loss: 0.5761337280273438\n",
      "Epoch 25/200, Step 0, Loss: 0.4121825695037842\n",
      "Epoch 26/200, Step 0, Loss: 0.46452832221984863\n",
      "Epoch 27/200, Step 0, Loss: 0.44031646847724915\n",
      "Epoch 28/200, Step 0, Loss: 0.3587251901626587\n",
      "Epoch 29/200, Step 0, Loss: 0.3438863158226013\n",
      "Epoch 30/200, Step 0, Loss: 0.24296054244041443\n",
      "Epoch 31/200, Step 0, Loss: 0.2383371889591217\n",
      "Epoch 32/200, Step 0, Loss: 0.22843292355537415\n",
      "Epoch 33/200, Step 0, Loss: 0.21390677988529205\n",
      "Epoch 34/200, Step 0, Loss: 0.18053244054317474\n",
      "Epoch 35/200, Step 0, Loss: 0.19323807954788208\n",
      "Epoch 36/200, Step 0, Loss: 0.19443579018115997\n",
      "Epoch 37/200, Step 0, Loss: 0.21486812829971313\n",
      "Epoch 38/200, Step 0, Loss: 0.20053154230117798\n",
      "Epoch 39/200, Step 0, Loss: 0.16489063203334808\n",
      "Epoch 40/200, Step 0, Loss: 0.11420886218547821\n",
      "Epoch 41/200, Step 0, Loss: 0.09554734826087952\n",
      "Epoch 42/200, Step 0, Loss: 0.13416984677314758\n",
      "Epoch 43/200, Step 0, Loss: 0.12338079512119293\n",
      "Epoch 44/200, Step 0, Loss: 0.09583751857280731\n",
      "Epoch 45/200, Step 0, Loss: 0.09937150776386261\n",
      "Epoch 46/200, Step 0, Loss: 0.095293790102005\n",
      "Epoch 47/200, Step 0, Loss: 0.12421418726444244\n",
      "Epoch 48/200, Step 0, Loss: 0.07551541924476624\n",
      "Epoch 49/200, Step 0, Loss: 0.07779857516288757\n",
      "Epoch 50/200, Step 0, Loss: 0.09171824157238007\n",
      "Epoch 51/200, Step 0, Loss: 0.056243397295475006\n",
      "Epoch 52/200, Step 0, Loss: 0.08259496837854385\n",
      "Epoch 53/200, Step 0, Loss: 0.09440860897302628\n",
      "Epoch 54/200, Step 0, Loss: 0.06807243078947067\n",
      "Epoch 55/200, Step 0, Loss: 0.058329835534095764\n",
      "Epoch 56/200, Step 0, Loss: 0.05915612354874611\n",
      "Epoch 57/200, Step 0, Loss: 0.04329606518149376\n",
      "Epoch 58/200, Step 0, Loss: 0.05641208216547966\n",
      "Epoch 59/200, Step 0, Loss: 0.06629082560539246\n",
      "Epoch 60/200, Step 0, Loss: 0.07795386761426926\n",
      "Epoch 61/200, Step 0, Loss: 0.05199328809976578\n",
      "Epoch 62/200, Step 0, Loss: 0.04129154235124588\n",
      "Epoch 63/200, Step 0, Loss: 0.05411922559142113\n",
      "Epoch 64/200, Step 0, Loss: 0.037310123443603516\n",
      "Epoch 65/200, Step 0, Loss: 0.040350817143917084\n",
      "Epoch 66/200, Step 0, Loss: 0.03915371745824814\n",
      "Epoch 67/200, Step 0, Loss: 0.037479933351278305\n",
      "Epoch 68/200, Step 0, Loss: 0.036671850830316544\n",
      "Epoch 69/200, Step 0, Loss: 0.041999101638793945\n",
      "Epoch 70/200, Step 0, Loss: 0.024723205715417862\n",
      "Epoch 71/200, Step 0, Loss: 0.03231858089566231\n",
      "Epoch 72/200, Step 0, Loss: 0.030128734186291695\n",
      "Epoch 73/200, Step 0, Loss: 0.030884424224495888\n",
      "Epoch 74/200, Step 0, Loss: 0.021566957235336304\n",
      "Epoch 75/200, Step 0, Loss: 0.027727652341127396\n",
      "Epoch 76/200, Step 0, Loss: 0.02633730135858059\n",
      "Epoch 77/200, Step 0, Loss: 0.039477601647377014\n",
      "Epoch 78/200, Step 0, Loss: 0.029936734586954117\n",
      "Epoch 79/200, Step 0, Loss: 0.0279228612780571\n",
      "Epoch 80/200, Step 0, Loss: 0.02950247749686241\n",
      "Epoch 81/200, Step 0, Loss: 0.02873021736741066\n",
      "Epoch 82/200, Step 0, Loss: 0.017248552292585373\n",
      "Epoch 83/200, Step 0, Loss: 0.02780269831418991\n",
      "Epoch 84/200, Step 0, Loss: 0.01769041270017624\n",
      "Epoch 85/200, Step 0, Loss: 0.031751107424497604\n",
      "Epoch 86/200, Step 0, Loss: 0.020996084436774254\n",
      "Epoch 87/200, Step 0, Loss: 0.01584203913807869\n",
      "Epoch 88/200, Step 0, Loss: 0.024350939318537712\n",
      "Epoch 89/200, Step 0, Loss: 0.02274276316165924\n",
      "Epoch 90/200, Step 0, Loss: 0.020473357290029526\n",
      "Epoch 91/200, Step 0, Loss: 0.012904590927064419\n",
      "Epoch 92/200, Step 0, Loss: 0.01513375248759985\n",
      "Epoch 93/200, Step 0, Loss: 0.01149684190750122\n",
      "Epoch 94/200, Step 0, Loss: 0.012240219861268997\n",
      "Epoch 95/200, Step 0, Loss: 0.012947294861078262\n",
      "Epoch 96/200, Step 0, Loss: 0.011278865858912468\n",
      "Epoch 97/200, Step 0, Loss: 0.013068443164229393\n",
      "Epoch 98/200, Step 0, Loss: 0.01558712963014841\n",
      "Epoch 99/200, Step 0, Loss: 0.009628279134631157\n",
      "Epoch 100/200, Step 0, Loss: 0.016751253977417946\n",
      "Epoch 101/200, Step 0, Loss: 0.009005213156342506\n",
      "Epoch 102/200, Step 0, Loss: 0.013156319037079811\n",
      "Epoch 103/200, Step 0, Loss: 0.009341013617813587\n",
      "Epoch 104/200, Step 0, Loss: 0.01088650245219469\n",
      "Epoch 105/200, Step 0, Loss: 0.012441970407962799\n",
      "Epoch 106/200, Step 0, Loss: 0.01437772624194622\n",
      "Epoch 107/200, Step 0, Loss: 0.01139776036143303\n",
      "Epoch 108/200, Step 0, Loss: 0.015488024801015854\n",
      "Epoch 109/200, Step 0, Loss: 0.0063029201701283455\n",
      "Epoch 110/200, Step 0, Loss: 0.008023890666663647\n",
      "Epoch 111/200, Step 0, Loss: 0.010124529711902142\n",
      "Epoch 112/200, Step 0, Loss: 0.008057261817157269\n",
      "Epoch 113/200, Step 0, Loss: 0.009219343774020672\n",
      "Epoch 114/200, Step 0, Loss: 0.01242963969707489\n",
      "Epoch 115/200, Step 0, Loss: 0.0051512508653104305\n",
      "Epoch 116/200, Step 0, Loss: 0.010125825181603432\n",
      "Epoch 117/200, Step 0, Loss: 0.005582872778177261\n",
      "Epoch 118/200, Step 0, Loss: 0.015942241996526718\n",
      "Epoch 119/200, Step 0, Loss: 0.004867808893322945\n",
      "Epoch 120/200, Step 0, Loss: 0.006288671400398016\n",
      "Epoch 121/200, Step 0, Loss: 0.005768687464296818\n",
      "Epoch 122/200, Step 0, Loss: 0.007795421406626701\n",
      "Epoch 123/200, Step 0, Loss: 0.005121472757309675\n",
      "Epoch 124/200, Step 0, Loss: 0.0068702902644872665\n",
      "Epoch 125/200, Step 0, Loss: 0.00916222296655178\n",
      "Epoch 126/200, Step 0, Loss: 0.00570826418697834\n",
      "Epoch 127/200, Step 0, Loss: 0.004234304651618004\n",
      "Epoch 128/200, Step 0, Loss: 0.004989337641745806\n",
      "Epoch 129/200, Step 0, Loss: 0.0047670709900557995\n",
      "Epoch 130/200, Step 0, Loss: 0.00920666754245758\n",
      "Epoch 131/200, Step 0, Loss: 0.0033277785405516624\n",
      "Epoch 132/200, Step 0, Loss: 0.0036493539810180664\n",
      "Epoch 133/200, Step 0, Loss: 0.008448025211691856\n",
      "Epoch 134/200, Step 0, Loss: 0.00524807209149003\n",
      "Epoch 135/200, Step 0, Loss: 0.006507338955998421\n",
      "Epoch 136/200, Step 0, Loss: 0.004515455104410648\n",
      "Epoch 137/200, Step 0, Loss: 0.005302732810378075\n",
      "Epoch 138/200, Step 0, Loss: 0.005471661686897278\n",
      "Epoch 139/200, Step 0, Loss: 0.0038779317401349545\n",
      "Epoch 140/200, Step 0, Loss: 0.0035824335645884275\n",
      "Epoch 141/200, Step 0, Loss: 0.0040743728168308735\n",
      "Epoch 142/200, Step 0, Loss: 0.004407922737300396\n",
      "Epoch 143/200, Step 0, Loss: 0.006077298894524574\n",
      "Epoch 144/200, Step 0, Loss: 0.0033899552654474974\n",
      "Epoch 145/200, Step 0, Loss: 0.0037184921093285084\n",
      "Epoch 146/200, Step 0, Loss: 0.002943507395684719\n",
      "Epoch 147/200, Step 0, Loss: 0.005652316380292177\n",
      "Epoch 148/200, Step 0, Loss: 0.0030537541024386883\n",
      "Epoch 149/200, Step 0, Loss: 0.003139442065730691\n",
      "Epoch 150/200, Step 0, Loss: 0.003411179641261697\n",
      "Epoch 151/200, Step 0, Loss: 0.002362665720283985\n",
      "Epoch 152/200, Step 0, Loss: 0.003867928171530366\n",
      "Epoch 153/200, Step 0, Loss: 0.002439080970361829\n",
      "Epoch 154/200, Step 0, Loss: 0.0022300779819488525\n",
      "Epoch 155/200, Step 0, Loss: 0.004030365496873856\n",
      "Epoch 156/200, Step 0, Loss: 0.002248947974294424\n",
      "Epoch 157/200, Step 0, Loss: 0.0018325800774618983\n",
      "Epoch 158/200, Step 0, Loss: 0.0030371418688446283\n",
      "Epoch 159/200, Step 0, Loss: 0.0018838122487068176\n",
      "Epoch 160/200, Step 0, Loss: 0.0020836316980421543\n",
      "Epoch 161/200, Step 0, Loss: 0.0044070761650800705\n",
      "Epoch 162/200, Step 0, Loss: 0.0015363183338195086\n",
      "Epoch 163/200, Step 0, Loss: 0.0018919834401458502\n",
      "Epoch 164/200, Step 0, Loss: 0.0020120725966989994\n",
      "Epoch 165/200, Step 0, Loss: 0.004407608415931463\n",
      "Epoch 166/200, Step 0, Loss: 0.002935085678473115\n",
      "Epoch 167/200, Step 0, Loss: 0.003228571731597185\n",
      "Epoch 168/200, Step 0, Loss: 0.001492982031777501\n",
      "Epoch 169/200, Step 0, Loss: 0.0031000161543488503\n",
      "Epoch 170/200, Step 0, Loss: 0.0017067886656150222\n",
      "Epoch 171/200, Step 0, Loss: 0.0012552724219858646\n",
      "Epoch 172/200, Step 0, Loss: 0.0019919537007808685\n",
      "Epoch 173/200, Step 0, Loss: 0.0013902934733778238\n",
      "Epoch 174/200, Step 0, Loss: 0.0020003048703074455\n",
      "Epoch 175/200, Step 0, Loss: 0.002408637199550867\n",
      "Epoch 176/200, Step 0, Loss: 0.001031946507282555\n",
      "Epoch 177/200, Step 0, Loss: 0.0018631541170179844\n",
      "Epoch 178/200, Step 0, Loss: 0.0024183986242860556\n",
      "Epoch 179/200, Step 0, Loss: 0.0019857739098370075\n",
      "Epoch 180/200, Step 0, Loss: 0.00399287324398756\n",
      "Epoch 181/200, Step 0, Loss: 0.001135209109634161\n",
      "Epoch 182/200, Step 0, Loss: 0.0014475302305072546\n",
      "Epoch 183/200, Step 0, Loss: 0.0012857414549216628\n",
      "Epoch 184/200, Step 0, Loss: 0.00113621074706316\n",
      "Epoch 185/200, Step 0, Loss: 0.0016611510654911399\n",
      "Epoch 186/200, Step 0, Loss: 0.0012332357000559568\n",
      "Epoch 187/200, Step 0, Loss: 0.0007311222143471241\n",
      "Epoch 188/200, Step 0, Loss: 0.001457637408748269\n",
      "Epoch 189/200, Step 0, Loss: 0.0012881086440756917\n",
      "Epoch 190/200, Step 0, Loss: 0.0009258410427719355\n",
      "Epoch 191/200, Step 0, Loss: 0.0008451610337942839\n",
      "Epoch 192/200, Step 0, Loss: 0.0007038957555778325\n",
      "Epoch 193/200, Step 0, Loss: 0.002718651667237282\n",
      "Epoch 194/200, Step 0, Loss: 0.0012771047186106443\n",
      "Epoch 195/200, Step 0, Loss: 0.0009813531069085002\n",
      "Epoch 196/200, Step 0, Loss: 0.0009177230531349778\n",
      "Epoch 197/200, Step 0, Loss: 0.0008969211485236883\n",
      "Epoch 198/200, Step 0, Loss: 0.0005862336256541312\n",
      "Epoch 199/200, Step 0, Loss: 0.0015819144900888205\n",
      "Epoch 200/200, Step 0, Loss: 0.0007453938596881926\n"
     ]
    }
   ],
   "source": [
    "def create_data(embeddings, vocab_size):\n",
    "    x = embeddings.weight.data.clone()\n",
    "    y = torch.eye(vocab_size)\n",
    "    return x, y\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "x, y = create_data(embedding_layer, vocab_size)\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class InverseEmbeddingMLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(InverseEmbeddingMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = InverseEmbeddingMLP(embedding_layer.embedding_dim, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Step {i}, Loss: {loss.item()}\")\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "for word in vocab:\n",
    "    if word in ['<SoS>', '<EoS>', '<null>']:\n",
    "        continue\n",
    "    word_index = word_to_index[word]\n",
    "    embedded_word = pretrained_embedding[word]\n",
    "    retrieved = model(torch.tensor(embedded_word))\n",
    "    predicted_indices = torch.argmax(retrieved, dim=0)\n",
    "    assert predicted_indices.item() == word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'inverse_embedding_tokens.pt' if with_tokens else 'inverse_embedding.pt'\n",
    "checkpoint_path = os.path.join(os.getcwd(),'inverse_embedding_tokens.pt')\n",
    "torch.save(model.state_dict(), checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmtcmr_copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
