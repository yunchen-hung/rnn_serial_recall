{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from nmt_cmr_parallels.data.sequence_data import load_cached_vocabulary, create_pretrained_semantic_embedding\n",
    "DEFAULT_DATA_PATH = os.path.expanduser(os.path.join('~', '.seq_nlp_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_tokens=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = create_pretrained_semantic_embedding(DEFAULT_DATA_PATH,50)\n",
    "vocab = load_cached_vocabulary(\"peers_vocab.json\")\n",
    "vocab = [x.lower() for x in vocab]\n",
    "if with_tokens:\n",
    "    vocab = ['<null>'] + vocab + ['<SoS>','<EoS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(len(vocab), 50)\n",
    "    \n",
    "# Initialize the weights of the Embedding layer with the GloVe vectors\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "for word, index in word_to_index.items():\n",
    "    if word in pretrained_embedding:\n",
    "        embedding_layer.weight.data[index] = torch.tensor(pretrained_embedding[word], dtype=torch.float32)\n",
    "\n",
    "# Freeze the embedding layer\n",
    "for param in embedding_layer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Step 0, Loss: 7.35943078994751\n",
      "Epoch 2/200, Step 0, Loss: 7.2550458908081055\n",
      "Epoch 3/200, Step 0, Loss: 7.099990367889404\n",
      "Epoch 4/200, Step 0, Loss: 6.924192428588867\n",
      "Epoch 5/200, Step 0, Loss: 6.613903999328613\n",
      "Epoch 6/200, Step 0, Loss: 6.2168121337890625\n",
      "Epoch 7/200, Step 0, Loss: 5.949657440185547\n",
      "Epoch 8/200, Step 0, Loss: 5.5308003425598145\n",
      "Epoch 9/200, Step 0, Loss: 5.156612396240234\n",
      "Epoch 10/200, Step 0, Loss: 4.712673187255859\n",
      "Epoch 11/200, Step 0, Loss: 4.3506317138671875\n",
      "Epoch 12/200, Step 0, Loss: 3.7388572692871094\n",
      "Epoch 13/200, Step 0, Loss: 3.209902763366699\n",
      "Epoch 14/200, Step 0, Loss: 2.731640338897705\n",
      "Epoch 15/200, Step 0, Loss: 2.4412717819213867\n",
      "Epoch 16/200, Step 0, Loss: 1.9692461490631104\n",
      "Epoch 17/200, Step 0, Loss: 1.6967265605926514\n",
      "Epoch 18/200, Step 0, Loss: 1.468616247177124\n",
      "Epoch 19/200, Step 0, Loss: 1.1560869216918945\n",
      "Epoch 20/200, Step 0, Loss: 0.9825842976570129\n",
      "Epoch 21/200, Step 0, Loss: 0.8363666534423828\n",
      "Epoch 22/200, Step 0, Loss: 0.6607023477554321\n",
      "Epoch 23/200, Step 0, Loss: 0.6730478405952454\n",
      "Epoch 24/200, Step 0, Loss: 0.6028921604156494\n",
      "Epoch 25/200, Step 0, Loss: 0.5034629702568054\n",
      "Epoch 26/200, Step 0, Loss: 0.40833842754364014\n",
      "Epoch 27/200, Step 0, Loss: 0.3922637701034546\n",
      "Epoch 28/200, Step 0, Loss: 0.3001178503036499\n",
      "Epoch 29/200, Step 0, Loss: 0.3181348443031311\n",
      "Epoch 30/200, Step 0, Loss: 0.24283480644226074\n",
      "Epoch 31/200, Step 0, Loss: 0.20917029678821564\n",
      "Epoch 32/200, Step 0, Loss: 0.23044973611831665\n",
      "Epoch 33/200, Step 0, Loss: 0.18078981339931488\n",
      "Epoch 34/200, Step 0, Loss: 0.2357027232646942\n",
      "Epoch 35/200, Step 0, Loss: 0.2173376977443695\n",
      "Epoch 36/200, Step 0, Loss: 0.13752222061157227\n",
      "Epoch 37/200, Step 0, Loss: 0.1539715975522995\n",
      "Epoch 38/200, Step 0, Loss: 0.1782180219888687\n",
      "Epoch 39/200, Step 0, Loss: 0.1279827058315277\n",
      "Epoch 40/200, Step 0, Loss: 0.13466349244117737\n",
      "Epoch 41/200, Step 0, Loss: 0.13055436313152313\n",
      "Epoch 42/200, Step 0, Loss: 0.11707864701747894\n",
      "Epoch 43/200, Step 0, Loss: 0.10661475360393524\n",
      "Epoch 44/200, Step 0, Loss: 0.09094904363155365\n",
      "Epoch 45/200, Step 0, Loss: 0.08831580728292465\n",
      "Epoch 46/200, Step 0, Loss: 0.1031498908996582\n",
      "Epoch 47/200, Step 0, Loss: 0.08825478702783585\n",
      "Epoch 48/200, Step 0, Loss: 0.08099208772182465\n",
      "Epoch 49/200, Step 0, Loss: 0.08223624527454376\n",
      "Epoch 50/200, Step 0, Loss: 0.07860071957111359\n",
      "Epoch 51/200, Step 0, Loss: 0.07338526099920273\n",
      "Epoch 52/200, Step 0, Loss: 0.08410346508026123\n",
      "Epoch 53/200, Step 0, Loss: 0.07622967660427094\n",
      "Epoch 54/200, Step 0, Loss: 0.0486019104719162\n",
      "Epoch 55/200, Step 0, Loss: 0.04891970753669739\n",
      "Epoch 56/200, Step 0, Loss: 0.06267238408327103\n",
      "Epoch 57/200, Step 0, Loss: 0.05189560353755951\n",
      "Epoch 58/200, Step 0, Loss: 0.05430908501148224\n",
      "Epoch 59/200, Step 0, Loss: 0.04829147458076477\n",
      "Epoch 60/200, Step 0, Loss: 0.04805993288755417\n",
      "Epoch 61/200, Step 0, Loss: 0.04183444008231163\n",
      "Epoch 62/200, Step 0, Loss: 0.05477713420987129\n",
      "Epoch 63/200, Step 0, Loss: 0.03687359020113945\n",
      "Epoch 64/200, Step 0, Loss: 0.032112084329128265\n",
      "Epoch 65/200, Step 0, Loss: 0.03475942835211754\n",
      "Epoch 66/200, Step 0, Loss: 0.04191721975803375\n",
      "Epoch 67/200, Step 0, Loss: 0.03851859271526337\n",
      "Epoch 68/200, Step 0, Loss: 0.04179670661687851\n",
      "Epoch 69/200, Step 0, Loss: 0.04898316040635109\n",
      "Epoch 70/200, Step 0, Loss: 0.0317361056804657\n",
      "Epoch 71/200, Step 0, Loss: 0.04373612999916077\n",
      "Epoch 72/200, Step 0, Loss: 0.03617243841290474\n",
      "Epoch 73/200, Step 0, Loss: 0.040286216884851456\n",
      "Epoch 74/200, Step 0, Loss: 0.031551893800497055\n",
      "Epoch 75/200, Step 0, Loss: 0.030864037573337555\n",
      "Epoch 76/200, Step 0, Loss: 0.028675075620412827\n",
      "Epoch 77/200, Step 0, Loss: 0.03591457009315491\n",
      "Epoch 78/200, Step 0, Loss: 0.027293357998132706\n",
      "Epoch 79/200, Step 0, Loss: 0.0317767970263958\n",
      "Epoch 80/200, Step 0, Loss: 0.022736284881830215\n",
      "Epoch 81/200, Step 0, Loss: 0.020085355266928673\n",
      "Epoch 82/200, Step 0, Loss: 0.025554779917001724\n",
      "Epoch 83/200, Step 0, Loss: 0.02260841801762581\n",
      "Epoch 84/200, Step 0, Loss: 0.018645716831088066\n",
      "Epoch 85/200, Step 0, Loss: 0.013080140575766563\n",
      "Epoch 86/200, Step 0, Loss: 0.02053152769804001\n",
      "Epoch 87/200, Step 0, Loss: 0.018879175186157227\n",
      "Epoch 88/200, Step 0, Loss: 0.0157462190836668\n",
      "Epoch 89/200, Step 0, Loss: 0.023646580055356026\n",
      "Epoch 90/200, Step 0, Loss: 0.018543194979429245\n",
      "Epoch 91/200, Step 0, Loss: 0.015490188263356686\n",
      "Epoch 92/200, Step 0, Loss: 0.01249984186142683\n",
      "Epoch 93/200, Step 0, Loss: 0.013251177966594696\n",
      "Epoch 94/200, Step 0, Loss: 0.022469662129878998\n",
      "Epoch 95/200, Step 0, Loss: 0.022935133427381516\n",
      "Epoch 96/200, Step 0, Loss: 0.01966315135359764\n",
      "Epoch 97/200, Step 0, Loss: 0.013991236686706543\n",
      "Epoch 98/200, Step 0, Loss: 0.013382161036133766\n",
      "Epoch 99/200, Step 0, Loss: 0.014107601717114449\n",
      "Epoch 100/200, Step 0, Loss: 0.017349660396575928\n",
      "Epoch 101/200, Step 0, Loss: 0.012570952996611595\n",
      "Epoch 102/200, Step 0, Loss: 0.013423090800642967\n",
      "Epoch 103/200, Step 0, Loss: 0.010340671986341476\n",
      "Epoch 104/200, Step 0, Loss: 0.007818734273314476\n",
      "Epoch 105/200, Step 0, Loss: 0.010934182442724705\n",
      "Epoch 106/200, Step 0, Loss: 0.009496727026998997\n",
      "Epoch 107/200, Step 0, Loss: 0.010678429156541824\n",
      "Epoch 108/200, Step 0, Loss: 0.01742580160498619\n",
      "Epoch 109/200, Step 0, Loss: 0.014743891544640064\n",
      "Epoch 110/200, Step 0, Loss: 0.008697949349880219\n",
      "Epoch 111/200, Step 0, Loss: 0.0055793272331357\n",
      "Epoch 112/200, Step 0, Loss: 0.0077272579073905945\n",
      "Epoch 113/200, Step 0, Loss: 0.010816860944032669\n",
      "Epoch 114/200, Step 0, Loss: 0.007121533155441284\n",
      "Epoch 115/200, Step 0, Loss: 0.007240321487188339\n",
      "Epoch 116/200, Step 0, Loss: 0.012989011593163013\n",
      "Epoch 117/200, Step 0, Loss: 0.012970604002475739\n",
      "Epoch 118/200, Step 0, Loss: 0.0103533323854208\n",
      "Epoch 119/200, Step 0, Loss: 0.007347073405981064\n",
      "Epoch 120/200, Step 0, Loss: 0.007612467277795076\n",
      "Epoch 121/200, Step 0, Loss: 0.006561451591551304\n",
      "Epoch 122/200, Step 0, Loss: 0.005452022887766361\n",
      "Epoch 123/200, Step 0, Loss: 0.00734673161059618\n",
      "Epoch 124/200, Step 0, Loss: 0.011832543648779392\n",
      "Epoch 125/200, Step 0, Loss: 0.0041213990189135075\n",
      "Epoch 126/200, Step 0, Loss: 0.0046469541266560555\n",
      "Epoch 127/200, Step 0, Loss: 0.0062952423468232155\n",
      "Epoch 128/200, Step 0, Loss: 0.0061314222402870655\n",
      "Epoch 129/200, Step 0, Loss: 0.004152341280132532\n",
      "Epoch 130/200, Step 0, Loss: 0.004591953940689564\n",
      "Epoch 131/200, Step 0, Loss: 0.0046462020836770535\n",
      "Epoch 132/200, Step 0, Loss: 0.0071997009217739105\n",
      "Epoch 133/200, Step 0, Loss: 0.006670682691037655\n",
      "Epoch 134/200, Step 0, Loss: 0.0080685680732131\n",
      "Epoch 135/200, Step 0, Loss: 0.004142550751566887\n",
      "Epoch 136/200, Step 0, Loss: 0.0039968667551875114\n",
      "Epoch 137/200, Step 0, Loss: 0.0036163367331027985\n",
      "Epoch 138/200, Step 0, Loss: 0.004455503076314926\n",
      "Epoch 139/200, Step 0, Loss: 0.0029558558017015457\n",
      "Epoch 140/200, Step 0, Loss: 0.004417479503899813\n",
      "Epoch 141/200, Step 0, Loss: 0.003246000735089183\n",
      "Epoch 142/200, Step 0, Loss: 0.004407411441206932\n",
      "Epoch 143/200, Step 0, Loss: 0.0026232744567096233\n",
      "Epoch 144/200, Step 0, Loss: 0.0032775201834738255\n",
      "Epoch 145/200, Step 0, Loss: 0.0026201289147138596\n",
      "Epoch 146/200, Step 0, Loss: 0.0030236414168030024\n",
      "Epoch 147/200, Step 0, Loss: 0.003541266079992056\n",
      "Epoch 148/200, Step 0, Loss: 0.0024939696304500103\n",
      "Epoch 149/200, Step 0, Loss: 0.0032011682633310556\n",
      "Epoch 150/200, Step 0, Loss: 0.004766235128045082\n",
      "Epoch 151/200, Step 0, Loss: 0.002595932222902775\n",
      "Epoch 152/200, Step 0, Loss: 0.0035539004020392895\n",
      "Epoch 153/200, Step 0, Loss: 0.0041190870106220245\n",
      "Epoch 154/200, Step 0, Loss: 0.003858721349388361\n",
      "Epoch 155/200, Step 0, Loss: 0.002633037744089961\n",
      "Epoch 156/200, Step 0, Loss: 0.004751563537865877\n",
      "Epoch 157/200, Step 0, Loss: 0.001787929330021143\n",
      "Epoch 158/200, Step 0, Loss: 0.0037416419945657253\n",
      "Epoch 159/200, Step 0, Loss: 0.0027707747649401426\n",
      "Epoch 160/200, Step 0, Loss: 0.003091957187280059\n",
      "Epoch 161/200, Step 0, Loss: 0.0023189415223896503\n",
      "Epoch 162/200, Step 0, Loss: 0.0020646625198423862\n",
      "Epoch 163/200, Step 0, Loss: 0.002256921026855707\n",
      "Epoch 164/200, Step 0, Loss: 0.0017976694507524371\n",
      "Epoch 165/200, Step 0, Loss: 0.0019618598744273186\n",
      "Epoch 166/200, Step 0, Loss: 0.0022740319836884737\n",
      "Epoch 167/200, Step 0, Loss: 0.0013085331302136183\n",
      "Epoch 168/200, Step 0, Loss: 0.0023341327905654907\n",
      "Epoch 169/200, Step 0, Loss: 0.001232745824381709\n",
      "Epoch 170/200, Step 0, Loss: 0.0014877868816256523\n",
      "Epoch 171/200, Step 0, Loss: 0.0014320178888738155\n",
      "Epoch 172/200, Step 0, Loss: 0.001259506680071354\n",
      "Epoch 173/200, Step 0, Loss: 0.0016188020817935467\n",
      "Epoch 174/200, Step 0, Loss: 0.0014378104824572802\n",
      "Epoch 175/200, Step 0, Loss: 0.0020871274173259735\n",
      "Epoch 176/200, Step 0, Loss: 0.002930575981736183\n",
      "Epoch 177/200, Step 0, Loss: 0.0010932718869298697\n",
      "Epoch 178/200, Step 0, Loss: 0.0018825010629370809\n",
      "Epoch 179/200, Step 0, Loss: 0.0022301403805613518\n",
      "Epoch 180/200, Step 0, Loss: 0.002769745886325836\n",
      "Epoch 181/200, Step 0, Loss: 0.002086752327159047\n",
      "Epoch 182/200, Step 0, Loss: 0.0033701241482049227\n",
      "Epoch 183/200, Step 0, Loss: 0.0022992417216300964\n",
      "Epoch 184/200, Step 0, Loss: 0.001149775693193078\n",
      "Epoch 185/200, Step 0, Loss: 0.0015454269014298916\n",
      "Epoch 186/200, Step 0, Loss: 0.0013076995965093374\n",
      "Epoch 187/200, Step 0, Loss: 0.0017689282540231943\n",
      "Epoch 188/200, Step 0, Loss: 0.001167395617812872\n",
      "Epoch 189/200, Step 0, Loss: 0.0013378366129472852\n",
      "Epoch 190/200, Step 0, Loss: 0.0011216914281249046\n",
      "Epoch 191/200, Step 0, Loss: 0.0009341214317828417\n",
      "Epoch 192/200, Step 0, Loss: 0.0014329255791381001\n",
      "Epoch 193/200, Step 0, Loss: 0.001096657244488597\n",
      "Epoch 194/200, Step 0, Loss: 0.0007434528088197112\n",
      "Epoch 195/200, Step 0, Loss: 0.0009735194616951048\n",
      "Epoch 196/200, Step 0, Loss: 0.0015604929067194462\n",
      "Epoch 197/200, Step 0, Loss: 0.001848591840825975\n",
      "Epoch 198/200, Step 0, Loss: 0.001653820276260376\n",
      "Epoch 199/200, Step 0, Loss: 0.0017770027043297887\n",
      "Epoch 200/200, Step 0, Loss: 0.000934730633161962\n"
     ]
    }
   ],
   "source": [
    "def create_data(embeddings, vocab_size):\n",
    "    x = embeddings.weight.data.clone()\n",
    "    y = torch.eye(vocab_size)\n",
    "    return x, y\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "x, y = create_data(embedding_layer, vocab_size)\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class InverseEmbeddingMLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(InverseEmbeddingMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = InverseEmbeddingMLP(embedding_layer.embedding_dim, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Step {i}, Loss: {loss.item()}\")\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "for word in vocab:\n",
    "    if word in ['<SoS>', '<EoS>', '<null>']:\n",
    "        continue\n",
    "    word_index = word_to_index[word]\n",
    "    embedded_word = pretrained_embedding[word]\n",
    "    retrieved = model(torch.tensor(embedded_word))\n",
    "    predicted_indices = torch.argmax(retrieved, dim=0)\n",
    "    assert predicted_indices.item() == word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'inverse_embedding_tokens.pt' if with_tokens else 'inverse_embedding.pt'\n",
    "checkpoint_path = os.path.join(os.getcwd(),'inverse_embedding_tokens.pt')\n",
    "torch.save(model.state_dict(), checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
